for(i in 1:M){
eff_groups[,i] <- groupids_out[,i]*zs_out[,i]
}
# Figure out number of unique groups in each iteration
groups_per_iter <- apply(eff_groups, 1, function(x){length(unique(x))-1})
# How much group-switching is going on?
groups_per_ind <- apply(eff_groups, 2, function(x){length(unique(x))})
# What's the pack size distribution?
pack_sizes <- matrix(unlist(apply(eff_groups, 1, function(x){table(factor(x,levels=0:G))})),ncol=G+1, byrow=T)
group_outputs[[i]] <- list(eff_groups=eff_groups, groups_per_iter=groups_per_iter, groups_per_ind=groups_per_ind, pack_sizes=pack_sizes)
}
lapply(group_outputs, function(x){mean(x$groups_per_iter)})
warnings()
mean(group_outputs[[1]]$groups_per_iter)
x1 <- group_outputs[[1]]
group_outputs <- list()
for(i in 1:nsims){
group_model_out <- savemods[[i]]
groupids_out <- group_model_out[,paste0("group[",1:M,"]")]
zs_out <- group_model_out[,paste0("z[",1:M,"]")]
eff_groups <- groupids_out
for(j in 1:M){
eff_groups[,j] <- groupids_out[,j]*zs_out[,j]
}
# Figure out number of unique groups in each iteration
groups_per_iter <- apply(eff_groups, 1, function(x){length(unique(x))-1})
# How much group-switching is going on?
groups_per_ind <- apply(eff_groups, 2, function(x){length(unique(x))})
# What's the pack size distribution?
pack_sizes <- matrix(unlist(apply(eff_groups, 1, function(x){table(factor(x,levels=0:G))})),ncol=G+1, byrow=T)
group_outputs[[i]] <- list(eff_groups=eff_groups, groups_per_iter=groups_per_iter, groups_per_ind=groups_per_ind, pack_sizes=pack_sizes)
}
lapply(group_outputs, function(x){mean(x$groups_per_iter)})
lapply(group_outputs, function(x){mean(x$pack_sizes)})
lapply(group_outputs, function(x){mean(x$groups_per_ind)})
library(nimble)
dim(eff_groups)
ts.plot(savemods[[1]][,"N"])
ts.plot(savemods[[1]][,"alpha"])
ts.plot(savemods[[1]][,"group[1]"])
ts.plot(savemods[[1]][,"sigma2_det"])
ts.plot(savemods[[1]][,"a0"])
lapply(savemods, function(x){mean(x[,"N"])})
mean(lapply(savemods, function(x){mean(x[,"N"])}))
mean(unlist(lapply(savemods, function(x){mean(x[,"N"])})))
mean(unlist(lapply(group_outputs, function(x){mean(x$groups_per_iter)})))
ls("groups")
ls(pat="groups")
month(as.Date("2018-10-01"))
months(as.Date("2018-10-01"))
2+2+14+6+6
fishers <- c(1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3)
bobcats <- c(5, 3, 4, 4, 5, 4, 0, 3, 1, 3, 5, 6, 2, 4, 4, 0, 3, 2, 6, 2, 1, 6, 5, 7, 3, 6, 2, 4, 3, 0)
mean(fishers)
var(fishers)
mean(bobcats)
var(bobcats)
mu.f <- mean(fishers)
(mu.f^2)*exp(-mu.f)/(factorial(2))  #by hand
dpois(2,mu.f)                     #using dpois(x, mu)
mu.b <- mean(bobcats)
sum(dpois(0:6, mu.b))  #by hand
ppois(6,mu.b)                     #using ppois(x, mu)
p.g1f <- 1-ppois(1,mu.b)
p.2b <- dpois(2,mu.b)
p.g1f*p.2b
birds <- read.csv("C:/Users/Robbie/Desktop/qsci381spr20/Lab4/Lab4_SkillsData.csv")
View(birds)
mean(birds$Barn.Swallow)
mean(birds$American.Robin)
qnorm(0.0668, 6.7,1.8)
1-pnorm(10, 6.7, 1.8)
dynocc_PP_sim <- function(nsites = 200, ntimes = 4, nyears = 1, psulength = 120, ssulength = psulength/ntimes, lambda1 = lambda, useprob, occprob){
lambda <- lambda1
# simulate y - counts at sites
y <- array(0, dim=c(nsites, ntimes)) # counts at sites
z_i <- numeric(nsites) # latent occupancy at sites
u_ib <- array(0, dim=c(nsites, ntimes)) # latent use at site i during time b
for(i in 1:nsites){
z_i[i] <- rbinom(1, size = 1, p = occprob)
for(b in 1:ntimes){
u_ib[i,b] <- rbinom(1, size = 1, p = useprob)
# if(sum(u_ib[i,])==0){
#   z_i[i] <- 0
# }
y[i,b] <- rpois(1, z_i[i]*u_ib[i,b]*lambda*ssulength)
}
}
L <- matrix(ssulength, nrow=nsites, ncol=ntimes)
# List of CTDO data as output
ctdo_in <- list(nsites = nsites, nbaittimes = ntimes, y=y, timebaited=L)
# ^ Add 0 at the beginning then test this!!!
# discrete-time detection history
nsubreps <- 3 # number of "survey occasions", K in the paper
subreplength <- ssulength/nsubreps
max_detects <- max(y) # Maximum number of detections in the data
# max_detects is used to fill parts of the time array t_ibk with NA's
t_ibk <- array(NA, dim=c(nsites, ntimes, max_detects)) # times of detections, to be sorted into survey occasions
x <- array(0, dim=c(nsites, ntimes, nsubreps))
for(i in 1:nsites){
for(b in 1:ntimes){
if(y[i,b]>0){
temp_t <- runif(y[i,b],0,ssulength) # Detection is a Poisson process so detections are distributed uniformly in time
t_ibk[i,b,] <- c(temp_t, rep(NA, max_detects-length(temp_t)))
temp <- cut(t_ibk[i,b,which(!is.na(t_ibk[i,b,]))], breaks = c(0,subreplength,2*subreplength,ssulength))
temp2 <- temp
x[i,b,] <- ifelse(table(temp2) > 0, 1, 0)
}
}
}
# DO data
do_in <- list(nsites = nsites, nbaittimes = ntimes, x = x)
# return data for continuous-time and discrete-time multi-scale models
return(list(ctdo_in = ctdo_in, do_in = do_in))
}
temp_data <- dynocc_PP_sim(nsites = 200, ntimes = 4, psulength = 120, lambda1 = 0.06, useprob=0.8, occprob = 0.8) # Simulate data
temp_data$ctdo_in$y
library(stringr)
library(lubridate)
library(jagsUI)
## Load data
load("C:/Users/Robbie/Desktop/wolverines/data/RforRobbie/RforRobbie/detections_Long.Rdata")
load("C:/Users/Robbie/Desktop/wolverines/data/RforRobbie/RforRobbie/cameras_Long.Rdata")
ntimes <- 4
time_scaler <- 1
# Reformat dates
cams.processed$ActiveStart <- as.POSIXct(cams.processed$ActiveStart, format = "%m/%d/%Y %H:%M:%S", tz = "US/Pacific")
cams.processed$ActiveEnd <- as.POSIXct(cams.processed$ActiveEnd, format = "%m/%d/%Y %H:%M:%S", tz = "US/Pacific")
dets.processed$ActiveStart <- as.POSIXct(dets.processed$ActiveStart, format = "%m/%d/%Y %H:%M:%S", tz = "US/Pacific")
dets.processed$ActiveEnd <- as.POSIXct(dets.processed$ActiveEnd, format = "%m/%d/%Y %H:%M:%S", tz = "US/Pacific")
dets.processed$ImageDate <- as.POSIXct(dets.processed$ImageDate, format = "%m/%d/%Y %H:%M:%S", tz = "US/Pacific")
# Create copies of ActiveStart and ActiveEnd for bait model data processing
dets.processed$ActiveStartOld <- dets.processed$ActiveStart
dets.processed$ActiveEndOld <- dets.processed$ActiveEnd
cams.processed$ActiveStartOld <- cams.processed$ActiveStart
cams.processed$ActiveEndOld <- cams.processed$ActiveEnd
# Get rid of camera rows that have no ActiveStart or ActiveEnd
empty_times <- which(is.na(cams.processed$ActiveStart))
cams.processed <- cams.processed[-empty_times,]
## WA only subset
wa.cams <- subset(cams.processed, state=="WA")
wa.dets <- subset(dets.processed, state=="WA")
cuttimes <- seq(ISOdate(2016, 12, 1, hour = 0, tz = "US/Pacific"), length.out = ntimes+1, by = "30 days")
cutseas <- 1:4
# Some auxiliary variables defining dimensions
nsites <- length(unique(wa.cams$LocationName))
ssulength <- 30
psulength <- ssulength*ntimes
L <- matrix(0, nrow=nsites, ncol=ntimes)
for(i in 1:nsites){
# Check time zones!
sitecams <- subset(wa.cams, LocationName==unique(wa.cams$LocationName)[i])
if(max(sitecams$start) < min(cuttimes) & min(sitecams$end) > max(cuttimes)){
L[i,] <- rep(ssulength, ntimes)
} else {
# this is if any part of the x-month survey period isn't sampled
# beginning time
L[i,1] <- ssulength-(max(sitecams$start) - min(cuttimes))
# middle times
L[i, 2:(ntimes-1)] <- ssulength
# end time
L[i, ntimes] <- min(as.numeric(ssulength-(max(cuttimes)-min(sitecams$end))), ssulength)
}
}
L
pnorm(325, 350, 37.5)-pnorm(300, 350, 37.5)
((1-0.08)^3)*0.08
dpois(5,2)
1/0.015
qnorm(0.2,0,1)
qnorm(0.6,0,1)
-0.842-0.253
5/1.095
15+0.842*4.566
setwd(“C:/Users/Robbie/Desktop/qsci381spr20/Lab5/”)
manaus <- read.csv(“manaus.csv”)
height <- manaus$Stage.Height
setwd("C:/Users/Robbie/Desktop/qsci381spr20/Lab5/")
manaus <- read.csv(“manaus.csv”)
height <- manaus$Stage.Height
setwd("C:/Users/Robbie/Desktop/qsci381spr20/Lab5/")
manaus <- read.csv("manaus.csv")
height <- manaus$Stage.Height
qqnorm(height)
abline(0,1)
qqnorm(height)
qqline(height)
zscores <- (height – mean(height))/sd(height)
zscores <- (height - mean(height))/sd(height)
probability <- pnorm(zscores, 0, 1, lower.tail=TRUE)
xprobs <- seq(0.01, 0.99,by=0.01)
heightquants <- qnorm(probability, mean(height), sd(height))
plot(xprobs, heightquants)
heightquants <- qnorm(xprobs, mean(height), sd(height))
plot(xprobs, heightquants)
mean(height)
sd(height)
install.packages(c("BayesianTools", "blmeco", "randomForest"))
#### Normal first then logit
library(MuMIn)
setwd("C:/Users/Robbie/Desktop/Model-Averaging")
dat.norm <-  read.csv("normalSimulatedData1April2020.csv")
#  Fit global model
fm1.norm <- glm(y~., data = dat.norm, na.action = "na.fail", family = "gaussian")
mytab <- dredge(fm1.norm, rank=AIC)
model.list <- get.models(mytab, subset=NA)
## Implement stacking function from Dormann et al. 2018 Supplementary Information
stacking <- function(test.preds, test.obs){
# this function computes the optimal weight for a single train/test split;
# from the models fitted to the training data it uses the predictions to the test;
# then it optimises the weight vector across the models for combining these
# predictions to the observed data in the test;
# trick 1: each weight is between 0 and 1: w <- exp(-w)
# trick 2: weights sum to 1: w <- w/sum(w)
#
# weights are weights for each model, between -infty and +infty!
# preds are predictions from each of the models
if (NCOL(test.preds) >= length(test.obs)) stop("Increase the test set! More models
than test points.")
# now do an internal splitting into "folds" data sets:
weightsopt <- function(ww){
# function to compute RMSE on test data
w <- c(1, exp(ww)); w <- w/sum(w) ## w all in (0,1) SIMON;
# set weight1 always to 1, other weights are scaled accordingly
# (this leads to a tiny dependence of optimal weights on whether model1 is any
# good or utter rubbish;
# see by moving the 1 to the end instead -> 3rd digit changes)
pred <- as.vector(test.preds %*% w)
return(sqrt(mean((pred - test.obs)^2)))
}
ops <- optim(par=runif(NCOL(test.preds)-1), weightsopt, method="BFGS")
if (ops$convergence != 0) stop("Optimisation not converged!")
round(c(1, exp(ops$par))/sum(c(1, exp(ops$par))), 4)
}
Nstack <- 10
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
while (i < Nstack){
trainsplit <- sample(rep(c(T, F), floor(N/2)))
trainsub1 <- dat.norm[trainsplit, ]
trainsub2 <- dat.norm[!trainsplit, ]
trainsub1fits <- lapply(model.list, update, .~. , data=trainsub1) # re-fit models on train1
stackpreds <- sapply(trainsub1fits, predict, newdata=trainsub2) # predict them to train2
optres <- try(stacking(test.preds=stackpreds, test.obs=trainsub2$y))
if (inherits(optres, "try-error"))  next;
i = i + 1
weightsStack[i,] <- optres
rm(trainsplit, trainsub1, trainsub2, trainsub1fits, stackpreds)
#print(i)
}
N <- nrow(dat.norm)
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
while (i < Nstack){
trainsplit <- sample(rep(c(T, F), floor(N/2)))
trainsub1 <- dat.norm[trainsplit, ]
trainsub2 <- dat.norm[!trainsplit, ]
trainsub1fits <- lapply(model.list, update, .~. , data=trainsub1) # re-fit models on train1
stackpreds <- sapply(trainsub1fits, predict, newdata=trainsub2) # predict them to train2
optres <- try(stacking(test.preds=stackpreds, test.obs=trainsub2$y))
if (inherits(optres, "try-error"))  next;
i = i + 1
weightsStack[i,] <- optres
rm(trainsplit, trainsub1, trainsub2, trainsub1fits, stackpreds)
#print(i)
}
?get.models
model.list <- get.models(mytab, subset=50)
model.list
model.list <- get.models(mytab, subset=1:50)
model.list$`192`
model.list$`448`
## Implement stacking function from Dormann et al. 2018 Supplementary Information
stacking <- function(test.preds, test.obs){
# this function computes the optimal weight for a single train/test split;
# from the models fitted to the training data it uses the predictions to the test;
# then it optimises the weight vector across the models for combining these
# predictions to the observed data in the test;
# trick 1: each weight is between 0 and 1: w <- exp(-w)
# trick 2: weights sum to 1: w <- w/sum(w)
#
# weights are weights for each model, between -infty and +infty!
# preds are predictions from each of the models
if (NCOL(test.preds) >= length(test.obs)) stop("Increase the test set! More models
than test points.")
# now do an internal splitting into "folds" data sets:
weightsopt <- function(ww){
# function to compute RMSE on test data
w <- c(1, exp(ww)); w <- w/sum(w) ## w all in (0,1) SIMON;
# set weight1 always to 1, other weights are scaled accordingly
# (this leads to a tiny dependence of optimal weights on whether model1 is any
# good or utter rubbish;
# see by moving the 1 to the end instead -> 3rd digit changes)
pred <- as.vector(test.preds %*% w)
return(sqrt(mean((pred - test.obs)^2)))
}
ops <- optim(par=runif(NCOL(test.preds)-1), weightsopt, method="BFGS")
if (ops$convergence != 0) stop("Optimisation not converged!")
round(c(1, exp(ops$par))/sum(c(1, exp(ops$par))), 4)
}
Nstack <- 10
N <- nrow(dat.norm)
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
weightsStack <- matrix(NA, ncol=50, nrow=Nstack)
M <- 50
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
while (i < Nstack){
trainsplit <- sample(rep(c(T, F), floor(N/2)))
trainsub1 <- dat.norm[trainsplit, ]
trainsub2 <- dat.norm[!trainsplit, ]
trainsub1fits <- lapply(model.list, update, .~. , data=trainsub1) # re-fit models on train1
stackpreds <- sapply(trainsub1fits, predict, newdata=trainsub2) # predict them to train2
optres <- try(stacking(test.preds=stackpreds, test.obs=trainsub2$y))
if (inherits(optres, "try-error"))  next;
i = i + 1
weightsStack[i,] <- optres
rm(trainsplit, trainsub1, trainsub2, trainsub1fits, stackpreds)
#print(i)
}
floor(N/2)
M <- 60
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
model.list <- get.models(mytab, subset=1:60)
## Implement stacking function from Dormann et al. 2018 Supplementary Information
stacking <- function(test.preds, test.obs){
# this function computes the optimal weight for a single train/test split;
# from the models fitted to the training data it uses the predictions to the test;
# then it optimises the weight vector across the models for combining these
# predictions to the observed data in the test;
# trick 1: each weight is between 0 and 1: w <- exp(-w)
# trick 2: weights sum to 1: w <- w/sum(w)
#
# weights are weights for each model, between -infty and +infty!
# preds are predictions from each of the models
if (NCOL(test.preds) >= length(test.obs)) stop("Increase the test set! More models
than test points.")
# now do an internal splitting into "folds" data sets:
weightsopt <- function(ww){
# function to compute RMSE on test data
w <- c(1, exp(ww)); w <- w/sum(w) ## w all in (0,1) SIMON;
# set weight1 always to 1, other weights are scaled accordingly
# (this leads to a tiny dependence of optimal weights on whether model1 is any
# good or utter rubbish;
# see by moving the 1 to the end instead -> 3rd digit changes)
pred <- as.vector(test.preds %*% w)
return(sqrt(mean((pred - test.obs)^2)))
}
ops <- optim(par=runif(NCOL(test.preds)-1), weightsopt, method="BFGS")
if (ops$convergence != 0) stop("Optimisation not converged!")
round(c(1, exp(ops$par))/sum(c(1, exp(ops$par))), 4)
}
Nstack <- 10
N <- nrow(dat.norm)
M <- 60
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
while (i < Nstack){
trainsplit <- sample(rep(c(T, F), floor(N/2)))
trainsub1 <- dat.norm[trainsplit, ]
trainsub2 <- dat.norm[!trainsplit, ]
trainsub1fits <- lapply(model.list, update, .~. , data=trainsub1) # re-fit models on train1
stackpreds <- sapply(trainsub1fits, predict, newdata=trainsub2) # predict them to train2
optres <- try(stacking(test.preds=stackpreds, test.obs=trainsub2$y))
if (inherits(optres, "try-error"))  next;
i = i + 1
weightsStack[i,] <- optres
rm(trainsplit, trainsub1, trainsub2, trainsub1fits, stackpreds)
#print(i)
}
model.list <- get.models(mytab, subset=1:40)
## Implement stacking function from Dormann et al. 2018 Supplementary Information
stacking <- function(test.preds, test.obs){
# this function computes the optimal weight for a single train/test split;
# from the models fitted to the training data it uses the predictions to the test;
# then it optimises the weight vector across the models for combining these
# predictions to the observed data in the test;
# trick 1: each weight is between 0 and 1: w <- exp(-w)
# trick 2: weights sum to 1: w <- w/sum(w)
#
# weights are weights for each model, between -infty and +infty!
# preds are predictions from each of the models
if (NCOL(test.preds) >= length(test.obs)) stop("Increase the test set! More models
than test points.")
# now do an internal splitting into "folds" data sets:
weightsopt <- function(ww){
# function to compute RMSE on test data
w <- c(1, exp(ww)); w <- w/sum(w) ## w all in (0,1) SIMON;
# set weight1 always to 1, other weights are scaled accordingly
# (this leads to a tiny dependence of optimal weights on whether model1 is any
# good or utter rubbish;
# see by moving the 1 to the end instead -> 3rd digit changes)
pred <- as.vector(test.preds %*% w)
return(sqrt(mean((pred - test.obs)^2)))
}
ops <- optim(par=runif(NCOL(test.preds)-1), weightsopt, method="BFGS")
if (ops$convergence != 0) stop("Optimisation not converged!")
round(c(1, exp(ops$par))/sum(c(1, exp(ops$par))), 4)
}
Nstack <- 10
N <- nrow(dat.norm)
M <- 40
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
while (i < Nstack){
trainsplit <- sample(rep(c(T, F), floor(N/2)))
trainsub1 <- dat.norm[trainsplit, ]
trainsub2 <- dat.norm[!trainsplit, ]
trainsub1fits <- lapply(model.list, update, .~. , data=trainsub1) # re-fit models on train1
stackpreds <- sapply(trainsub1fits, predict, newdata=trainsub2) # predict them to train2
optres <- try(stacking(test.preds=stackpreds, test.obs=trainsub2$y))
if (inherits(optres, "try-error"))  next;
i = i + 1
weightsStack[i,] <- optres
rm(trainsplit, trainsub1, trainsub2, trainsub1fits, stackpreds)
#print(i)
}
## Calculate weights
(weightsStacking <- colSums(weightsStack)/sum(weightsStack))
View(dat.norm)
truth <- testdat$y
N <- nrow(dat.norm)
Ntrain <- N/2
datsplit <- sample(rep(c(T,F),floor(N/2)))
train <- dat.norm[datsplit,]
testdat <- dat.norm[!datsplit,]
truth <- testdat$y
preds <- sapply(model.list, predict, newdata=testdat)
setwd("C:/Users/Robbie/Desktop/Model-Averaging")
dat.norm <-  read.csv("normalSimulatedData1April2020.csv")
#  Fit global model
fm1.norm <- glm(y~., data = dat.norm, na.action = "na.fail", family = "gaussian")
mytab <- dredge(fm1.norm, rank=AIC)
model.list <- get.models(mytab, subset=1:20)
# Reorder model list?
# Maybe get only top 50 models vel sim.
## Implement stacking function from Dormann et al. 2018 Supplementary Information
stacking <- function(test.preds, test.obs){
# this function computes the optimal weight for a single train/test split;
# from the models fitted to the training data it uses the predictions to the test;
# then it optimises the weight vector across the models for combining these
# predictions to the observed data in the test;
# trick 1: each weight is between 0 and 1: w <- exp(-w)
# trick 2: weights sum to 1: w <- w/sum(w)
#
# weights are weights for each model, between -infty and +infty!
# preds are predictions from each of the models
if (NCOL(test.preds) >= length(test.obs)) stop("Increase the test set! More models
than test points.")
# now do an internal splitting into "folds" data sets:
weightsopt <- function(ww){
# function to compute RMSE on test data
w <- c(1, exp(ww)); w <- w/sum(w) ## w all in (0,1) SIMON;
# set weight1 always to 1, other weights are scaled accordingly
# (this leads to a tiny dependence of optimal weights on whether model1 is any
# good or utter rubbish;
# see by moving the 1 to the end instead -> 3rd digit changes)
pred <- as.vector(test.preds %*% w)
return(sqrt(mean((pred - test.obs)^2)))
}
ops <- optim(par=runif(NCOL(test.preds)-1), weightsopt, method="BFGS")
if (ops$convergence != 0) stop("Optimisation not converged!")
round(c(1, exp(ops$par))/sum(c(1, exp(ops$par))), 4)
}
Nstack <- 10
N <- nrow(dat.norm)
Ntrain <- N/2
datsplit <- sample(rep(c(T,F),floor(N/2)))
train <- dat.norm[datsplit,]
testdat <- dat.norm[!datsplit,]
M <- 20
weightsStack <- matrix(NA, ncol=M, nrow=Nstack)
colnames(weightsStack) <- paste0("m", 1:M)
i = 0
while (i < Nstack){
trainsplit <- sample(rep(c(T, F), floor(Ntrain/2)))
trainsub1 <- train[trainsplit, ]
trainsub2 <- train[!trainsplit, ]
trainsub1fits <- lapply(model.list, update, .~. , data=trainsub1) # re-fit models on train1
stackpreds <- sapply(trainsub1fits, predict, newdata=trainsub2) # predict them to train2
optres <- try(stacking(test.preds=stackpreds, test.obs=trainsub2$y))
if (inherits(optres, "try-error"))  next;
i = i + 1
weightsStack[i,] <- optres
rm(trainsplit, trainsub1, trainsub2, trainsub1fits, stackpreds)
#print(i)
}
## Calculate weights
(weightsStacking <- colSums(weightsStack)/sum(weightsStack))
preds <- sapply(model.list, predict, newdata=testdat)
truth <- testdat$y
weightedPredsStack <- preds %*% weightsStacking # Add back in preds
(RMSEstack <- sqrt(mean((weightedPredsStack - truth)^2))) # Add back in truth
fishers <- c(1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3)
bobcats <- c(5, 3, 4, 4, 5, 4, 0, 3, 1, 3, 5, 6, 2, 4, 4, 0, 3, 2, 6, 2, 1, 6, 5, 7, 3, 6, 2, 4, 3, 0)
mu.f <- mean(fishers)
mu.b <- mean(bobcats)
p.g1f <- 1-ppois(1,mu.b)
p.2b <- dpois(2,mu.b)
p.g1f*p.2b
p.g1f + p.2b - p.g1f*p.2b
p.g1f <- 1-ppois(1,mu.f)
p.2b <- dpois(2,mu.b)
p.g1f*p.2b
p.g1f + p.2b - p.g1f*p.2b
